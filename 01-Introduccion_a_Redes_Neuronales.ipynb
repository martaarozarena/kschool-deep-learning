{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "redes-neuronales.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_53qspkYN0ha",
        "colab_type": "text"
      },
      "source": [
        "# Introducción básica a Redes Neuronales\n",
        "\n",
        "Vamos a usar Scikit-learn para tomar contacto con las redes neuronales. Aunque no se recomienda para hacer Deep Learning, porque no tiene soporte de GPUs, nos va a servir para tomar contacto inicialmente con redes neuronales con un interfaz que ya conocemos de las clases de Supervised Learning.\n",
        "\n",
        "Primero, cargamos los módulos necesarios:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqD5c3gGoKGl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "plt.rcParams[\"savefig.dpi\"] = 300\n",
        "plt.rcParams[\"savefig.bbox\"] = \"tight\"\n",
        "np.set_printoptions(precision=3, suppress=True)\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import scale, StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emMEJBZqPNXd",
        "colab_type": "text"
      },
      "source": [
        "## Clasificación\n",
        "\n",
        "\n",
        "Cargamos los módulos necesario de redes neuronales de Scikit-Learn y también cargamos el dataset de pruebas Make Moons para hacer una prueba de clasificación y lo pintamos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpKNqH3koKGp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "X, y = make_moons(n_samples=100, noise=0.25, random_state=2)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n",
        "                                                    random_state=42)\n",
        "\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=plt.cm.tab10(y_train))\n",
        "xlim = plt.xlim()\n",
        "ylim = plt.ylim()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRv-gDHSoKGs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xs = np.linspace(xlim[0], xlim[1], 1000)\n",
        "ys = np.linspace(ylim[0], ylim[1], 1000)\n",
        "xx, yy = np.meshgrid(xs, ys)\n",
        "X_grid = np.c_[xx.ravel(), yy.ravel()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydgeZjIgPEGf",
        "colab_type": "text"
      },
      "source": [
        "Por defecto, esto está utilizando una no linealidad con ReLU, pero trozo a trozo la frontera de decisión será lineal. Ya que el dataset es muy pequeño hemos puesto como solver l-bfgs aunque por defecto el solver es adam:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0LNtH6uoKGu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp = MLPClassifier(solver='lbfgs', random_state=0).fit(X_train, y_train)\n",
        "print(mlp.score(X_train, y_train))\n",
        "print(mlp.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWn24QIqQnRi",
        "colab_type": "text"
      },
      "source": [
        "Pintamos la frontera de decisión"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1E1QkJeoKGw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.contour(xx, yy, mlp.predict_proba(X_grid)[:, 1].reshape(xx.shape), levels=[.5])\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=plt.cm.tab10(y_train))\n",
        "\n",
        "plt.xlim(xlim)\n",
        "plt.ylim(ylim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVYAYd3CQybG",
        "colab_type": "text"
      },
      "source": [
        "Como ya hemos dicho, ya que se trata de un problema no convexo el resultado dependerá de la inicialización (pese a que el solver elegido es bastante robusto). Vamos a comprobarlo introducciones variación en el random state utilizado:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEWe8vMJoKGy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, axes = plt.subplots(3, 3, figsize=(8, 5))\n",
        "for ax, i in zip(axes.ravel(), range(10)):\n",
        "    mlp = MLPClassifier(solver='lbfgs', random_state=i, max_iter=1000).fit(X_train, y_train)\n",
        "    print(mlp.score(X_train, y_train))\n",
        "    print(mlp.score(X_test, y_test))\n",
        "\n",
        "    ax.contour(xx, yy, mlp.predict_proba(X_grid)[:, 1].reshape(xx.shape), levels=[.5])\n",
        "    ax.scatter(X_train[:, 0], X_train[:, 1], c=plt.cm.tab10(y_train))\n",
        "\n",
        "    ax.set_xlim(xlim)\n",
        "    ax.set_ylim(ylim)\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnMgXC0cRny8",
        "colab_type": "text"
      },
      "source": [
        "Esta red tiene por defecto una única capa de 100 nodos, con lo que la red es demasiado flexible para un dataset tan pequeño y lo que está haciendo es un sobreajuste para todos los diversos random states que hemos utilizado.\n",
        "\n",
        "Vamos a reducir tamaño de las capas ocultas (3 en este caso) para reducir el grado de sobreajuste que estamos viendo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_G0Saz1NoKG0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(10, 10, 10), random_state=0)\n",
        "mlp.fit(X_train, y_train)\n",
        "print(mlp.score(X_train, y_train))\n",
        "print(mlp.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jh0k5BgrTbv2",
        "colab_type": "text"
      },
      "source": [
        "En este caso, los tamaños de las matrices de pesos son consecutivamente 2x10, 10x10, 10x10 y 10x1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNuHBiiHoKG3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.contour(xx, yy, mlp.predict_proba(X_grid)[:, 1].reshape(xx.shape), levels=[.5])\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=plt.cm.tab10(y_train))\n",
        "\n",
        "plt.xlim(xlim)\n",
        "plt.ylim(ylim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-m5_VEnzSRIR",
        "colab_type": "text"
      },
      "source": [
        "En el gráfico pueden apreciarse los 10 segmentos correspondientes a los diez nodos que tienen las capas ocultas de la red neuronal. Como hemos dicho, no suele ser así como se configuran redes neuronales: típicamente las arquitecturas son mucho más complejas como para que la red pueda aprender funciones arbitrariamente complejas y que hacen que a la vez la red neuronal pueda generalizar suficientemente bien.\n",
        "\n",
        "\n",
        "Si utilizamos una función de activación `tanh` se puede ver que los resultados son mucho más suaves:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5ZswmgkS1Z3",
        "colab_type": "text"
      },
      "source": [
        "También podemos cambiar la función de activación, "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6pWIZdzoKG5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(10, 10, 10), activation=\"tanh\", random_state=0)\n",
        "mlp.fit(X_train, y_train)\n",
        "print(mlp.score(X_train, y_train))\n",
        "print(mlp.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73NBQrWpoKG8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.contour(xx, yy, mlp.predict_proba(X_grid)[:, 1].reshape(xx.shape), levels=[.5])\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=plt.cm.tab10(y_train))\n",
        "\n",
        "plt.xlim(xlim)\n",
        "plt.ylim(ylim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FgjadKLg9b9",
        "colab_type": "text"
      },
      "source": [
        "En aplicaciones reales, la suavidad de la frontera de decisión no importa tanto. Importa más tener una *learning rate* más alta, por ejemplo, y los data scientist utilizarán preferentemente una activación ReLU. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4H9D1zUhenz",
        "colab_type": "text"
      },
      "source": [
        "## Regresión\n",
        "\n",
        "Abordamos ahora un problema de regresión creado un dataset que sigue una función con aleatoriedad en las muestras, para ver qué tal predice la función original la red neuronal:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMCcm3KUoKG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rng = np.random.RandomState(0)\n",
        "x = np.sort(rng.uniform(size=100))\n",
        "y = np.sin(10 * x) + 5 * x + np.random.normal(0, .3, size=100)\n",
        "plt.plot(x, y, 'o')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvkFLtOGoKHA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "line = np.linspace(0, 1, 100)\n",
        "X = x.reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlNV-wJSiODW",
        "colab_type": "text"
      },
      "source": [
        "Aquí estamos utilizando un MLPRegressor (Multilayer Perceptron Regressor), que optimiz el error cuadrádrico utilizando o l-bfgs, *stochastic gradient descent* o la opción por defecto, *adam*. En este caso, debido al tamaño del dataset elegimos l-bfgs, y configuramos dos modelos para cada tipo de activación (`relu` o `tanh`):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RNA3Ie0oKHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "mlp_relu = MLPRegressor(solver=\"lbfgs\", hidden_layer_sizes=(10, 10, 10), max_iter=4000).fit(X, y)\n",
        "mlp_tanh = MLPRegressor(solver=\"lbfgs\", hidden_layer_sizes=(10, 10, 10), activation='tanh',  max_iter=4000).fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOB6OtMGoKHD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(x, y, 'o')\n",
        "plt.plot(line, mlp_relu.predict(line.reshape(-1, 1)), label=\"relu\")\n",
        "plt.plot(line, mlp_tanh.predict(line.reshape(-1, 1)), label=\"tanh\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSBDvmCKkg4I",
        "colab_type": "text"
      },
      "source": [
        "# Control de la complejidad\n",
        "\n",
        "Primero, ver las slides para la introducción de algunos conceptos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjT2D8Ajpspl",
        "colab_type": "text"
      },
      "source": [
        "Para mostrar la influencia de los parámetros sobre la red neuronal vamos a hacer un grid search de Scikit-Learn y utilizar diferentes parámetros.\n",
        "\n",
        "Seleccionamos para la prueba el Breast Cancer Dataset, con el que también trabajaremos después con Keras:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPqOUzP9oKHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUQYVdAirGI0",
        "colab_type": "text"
      },
      "source": [
        "Preparamos los datos, estratificándolos y utilizando un standard scaler:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9nkKF2MoKHL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data.data, data.target, stratify=data.target, random_state=0)\n",
        "scaler = StandardScaler().fit(X_train)\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NM4mK8VprQeZ",
        "colab_type": "text"
      },
      "source": [
        "Configuramos y entrenamos una Multilayer Perceptron Network utilizando los valores por defecto (una sola capa, con 100 nodos, adam optimizer y activación ReLu):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXvZim1AoKHP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp = MLPClassifier(max_iter=1000, random_state=0).fit(X_train_scaled, y_train)\n",
        "print(mlp.score(X_train_scaled, y_train))\n",
        "print(mlp.score(X_test_scaled, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKi7Kd6QoKHR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp = MLPClassifier(solver=\"lbfgs\", random_state=1).fit(X_train_scaled, y_train)\n",
        "print(mlp.score(X_train_scaled, y_train))\n",
        "print(mlp.score(X_test_scaled, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAYyjAT_pV5g",
        "colab_type": "text"
      },
      "source": [
        "El parámetro de regularización (*weight decay*) se llama *alpha* como en Ridge. Hacemos un grid search sobre este parámetro utilizando las pipelines de Scikit-learn para ver si encontramos un buen valor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4GbdRfRoKHT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "pipe = make_pipeline(StandardScaler(), MLPClassifier(max_iter=2000, solver=\"lbfgs\", random_state=1))\n",
        "param_grid = {'mlpclassifier__alpha': np.logspace(-3, 3, 7)}\n",
        "grid = GridSearchCV(pipe, param_grid, return_train_score=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhHAlmdloKHV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvJ160Gds7MR",
        "colab_type": "text"
      },
      "source": [
        "Almacenamos los resultados del grid search en un dataframe de pandas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b6RMgfOoKHX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = pd.DataFrame(grid.cv_results_)\n",
        "res = results.pivot_table(index=\"param_mlpclassifier__alpha\",\n",
        "                          values=[\"mean_test_score\", \"mean_train_score\"])\n",
        "res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2jr8-ZNs__S",
        "colab_type": "text"
      },
      "source": [
        "Pintamos el resultado:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsmijQmHoKHa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "res.plot()\n",
        "plt.xscale(\"log\")\n",
        "plt.ylim(0.95, 1.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0ctFkwT1zxG",
        "colab_type": "text"
      },
      "source": [
        "Como era de esperar, la precisión de training baja con el incremento de regularización. Vamos a incluir unas barras de error sobre capa punto para poder discutir mejor el problema:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTrvroryoKHc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "res = results.pivot_table(index=\"param_mlpclassifier__alpha\",\n",
        "                          values=[\"mean_test_score\",\n",
        "                                  \"mean_train_score\",\n",
        "                                  \"std_test_score\",\n",
        "                                  \"std_train_score\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_n-Yq1JoKHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "res.mean_test_score.plot(yerr=res.std_test_score)\n",
        "res.mean_train_score.plot(yerr=res.std_train_score)\n",
        "plt.xscale(\"log\")\n",
        "plt.ylim(0.95, 1.01)\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hLhByI028G4",
        "colab_type": "text"
      },
      "source": [
        "Las barras de error son bastante grandes, pero pese a eso parece que el óptimo está para un valor de *alpha* de 10, con una media más grande aunque con un poco más de desviación estándar que si hubiésemos un valor de *alpha* de $10^{-2}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mz1FJWLq3vFL",
        "colab_type": "text"
      },
      "source": [
        "También podemos hacer un grid search sobre el tamaño de las capas ocultas (podríamos buscar ambas a la vez tmabién):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkV5KeYloKHg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "pipe = make_pipeline(StandardScaler(), MLPClassifier(solver=\"lbfgs\", random_state=1))\n",
        "param_grid = {'mlpclassifier__hidden_layer_sizes':\n",
        "              [(10,), (50,), (100,), (500,), (10, 10), (50, 50), (100, 100), (500, 500)]\n",
        "             }\n",
        "grid = GridSearchCV(pipe, param_grid, return_train_score=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfDR9Qt63_8m",
        "colab_type": "text"
      },
      "source": [
        "Nada obliga a elegir diferentes capas con el mismo tamaño en número de nodos, aunque en datos tabulados es lo que se suele hacer porque simplifica algo las cosas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ju09rr1koKHi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNnyvyyZoKHj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = pd.DataFrame(grid.cv_results_)\n",
        "res = results.pivot_table(index=\"param_mlpclassifier__hidden_layer_sizes\",\n",
        "                          values=[\"mean_test_score\",\n",
        "                                  \"mean_train_score\",\n",
        "                                  \"std_test_score\",\n",
        "                                  \"std_train_score\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzGzqKHaoKHm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "res.mean_test_score.plot(yerr=res.std_test_score)\n",
        "res.mean_train_score.plot(yerr=res.std_train_score)\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qv10w4Ps6dDQ",
        "colab_type": "text"
      },
      "source": [
        "Lo que puede verse en este caso es que 100 nodos es lo que parece mejor, y en cualquier caso funciona mejor una configuración con una sola capa oculta en vez de dos.\n",
        "\n",
        "Esto son pruebas *de juguete* para entender los conceptos. Scikit-Learn no escala demasiado bien; para un minidataset como éste está bien, pero para cosas más serias (por ejemplo, el procesado de MNIST) no lo vamos a usar y pasaremos a otros frameworks que sí son utilizados de manera profesional."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgiG1AbL7NKx",
        "colab_type": "text"
      },
      "source": [
        "Ahora, volver a las transparencias de la presentación para una recapitulació del apartado de control de complejiad."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjSfSfaooKH4",
        "colab_type": "text"
      },
      "source": [
        "# Más allá de Scikit-Learn\n",
        "\n",
        "Se incluyen aquí la definición de un par de clases que implementan de una manera básica el *forward* pass y el *backward* pass de una red neuronal, así como dos operaciones de grafo computacional, una unión (enrutador de gradiente) y una multiplicación (conmutador ponderado de gradiente):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCz1tmxtoKH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuralNetwork(object):\n",
        "    def __init__(self):\n",
        "        # initialize coefficients and biases\n",
        "        pass\n",
        "    def forward(self, x):\n",
        "        activation = x\n",
        "        for coef, bias in zip(self.coef_, self.bias_):\n",
        "            activation = self.nonlinearity(np.dot(activation, coef) + bias)\n",
        "        return activation\n",
        "    def backward(self, x):\n",
        "        # compute gradient of stuff in forward pass\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAp50DxQoKH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# http://mxnet.io/architecture/program_model.html\n",
        "class array(object) :\n",
        "    \"\"\"Simple Array object that support autodiff.\"\"\"\n",
        "    def __init__(self, value, name=None):\n",
        "        self.value = value\n",
        "        if name:\n",
        "            self.grad = lambda g : {name : g}\n",
        "\n",
        "    def __add__(self, other):\n",
        "        assert isinstance(other, int)\n",
        "        ret = array(self.value + other)\n",
        "        ret.grad = lambda g : self.grad(g)\n",
        "        return ret\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        assert isinstance(other, array)\n",
        "        ret = array(self.value * other.value)\n",
        "        def grad(g):\n",
        "            x = self.grad(g * other.value)\n",
        "            x.update(other.grad(g * self.value))\n",
        "            return x\n",
        "        ret.grad = grad\n",
        "        return ret"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8P9MihnoKH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# some examples\n",
        "a = array(np.array([1, 2]), 'a')\n",
        "b = array(np.array([3, 4]), 'b')\n",
        "c = b * a\n",
        "d = c + 1\n",
        "print(d.value)\n",
        "print(d.grad(1))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}